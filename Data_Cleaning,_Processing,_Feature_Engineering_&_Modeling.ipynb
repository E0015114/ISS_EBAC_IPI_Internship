{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53UPZ2q21ef2",
        "outputId": "38efb1ab-08ee-4488-fc41-ea2f4b7f908f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading initial tables from BigQuery...\n",
            "Initial tables loaded.\n",
            "\n",
            "--- Running Cell 2: Load and Merge Excel Data into df_leads ---\n",
            "Successfully loaded data from 'Leads.xlsx'. Shape: (47627, 6)\n",
            "Merge complete. df_leads new shape: (47627, 25)\n",
            "Newly added/updated columns from Excel: ['ConvertedOpportunityId', 'IsConverted', 'ConvertedContactId', 'ConvertedDate', 'ConvertedAccountId']\n",
            "\n",
            "'IsConverted' column (from Excel) is now in df_leads.\n",
            "Value counts for 'IsConverted' (from Excel) in df_leads:\n",
            "IsConverted\n",
            "False    46177\n",
            "True      1450\n",
            "Name: count, dtype: int64\n",
            "\n",
            "'ConvertedAccountId' (from Excel) is now in df_leads.\n",
            "\n",
            "--- Running Cell 3: Convert String NaNs ---\n",
            "\n",
            "--- Running Cell 4: Define Columns to Drop ---\n",
            "\n",
            "--- Dropping Columns from: Company Accounts ---\n",
            "Original shape: (5631, 28)\n",
            "Columns in 'Company Accounts' BEFORE drop attempt: ['Id', 'About_the_Company__c', 'Areas_of_Interest__c', 'Name', 'Company_Profile__c', 'RecordTypeId', 'Country__c', 'CreatedDate', 'IAP__c', 'IPI_Remarks__c', 'IPI_WAR_Rating__c', 'Main_Products_Services__c', 'Nature_of_Business__c', 'Other_Sources__c', 'Owned_Patents__c', 'ParentId', 'Industry', 'Primary_Industry__c', 'Other_Primary_Industry__c', 'Ready__c', 'Revenue__c', 'AccountSource', 'Lead_Source__c', 'OwnerId', 'Tech_Seeker_Type__c', 'Tech_Seeker__c', 'Year_of_First_Engagement__c', 'Year_of_Last_Engagement__c']\n",
            "Dropped columns: ['About_the_Company__c', 'Areas_of_Interest__c', 'IPI_Remarks__c', 'IPI_WAR_Rating__c', 'Main_Products_Services__c', 'Other_Sources__c', 'ParentId', 'Other_Primary_Industry__c', 'Ready__c', 'Revenue__c', 'Tech_Seeker_Type__c', 'Year_of_Last_Engagement__c']\n",
            "New shape: (5631, 16)\n",
            "\n",
            "--- Dropping Columns from: Leads ---\n",
            "Original shape: (47627, 25)\n",
            "Columns in 'Leads' BEFORE drop attempt: ['Id', 'Address_country', 'Areas_of_Interest__c', 'Company_Classification__c', 'Company', 'Company_Profile__c', 'CreatedDate', 'Drop_Reason__c', 'Drop_Remarks__c', 'IAP__c', 'IPI_Remarks__c', 'Lead_RT_Name__c', 'Industry', 'LeadSource', 'Status', 'Tech_Expert_Name__c', 'Tech_Need_Title__c', 'Tech_Offer_Title__c', 'Type_of_Enquiry__c', 'Type_of_Service__c', 'IsConverted', 'ConvertedAccountId', 'ConvertedContactId', 'ConvertedDate', 'ConvertedOpportunityId']\n",
            "Dropped columns: ['Areas_of_Interest__c', 'Company_Classification__c', 'Company_Profile__c', 'Drop_Reason__c', 'Drop_Remarks__c', 'IPI_Remarks__c', 'Industry', 'Tech_Expert_Name__c', 'Tech_Need_Title__c', 'Tech_Offer_Title__c', 'Type_of_Enquiry__c', 'Type_of_Service__c']\n",
            "New shape: (47627, 13)\n",
            "\n",
            "--- Dropping Columns from: Contacts ---\n",
            "Original shape: (7374, 19)\n",
            "Columns in 'Contacts' BEFORE drop attempt: ['Id', 'Approved__c', 'Approver__c', 'Areas_of_Interest__c', 'AccountId', 'Country__c', 'CreatedDate', 'CventEvents__ContactStub__c', 'Cvent_Reference_ID__c', 'Cvent_Registration_Type__c', 'HasOptedOutOfEmail', 'Featured_in_Innovation_insights__c', 'Featured_in_Success_Stories__c', 'Innovation_Advisor__c', 'OwnerId', 'Potential_IA__c', 'RecordTypeId', 'LeadSource', 'Source__c']\n",
            "Dropped columns: ['Country__c', 'CventEvents__ContactStub__c', 'Cvent_Reference_ID__c', 'Cvent_Registration_Type__c', 'Source__c']\n",
            "New shape: (7374, 14)\n",
            "\n",
            "--- Processing DataFrame for Keep: Projects ---\n",
            "Identified columns to keep in Projects: ['Created_From_Lead__c', 'Id', 'IsWon']\n",
            "New shape of Projects: (4182, 3)\n",
            "\n",
            "--- Dropping Columns from: Meeting Notes ---\n",
            "Original shape: (3181, 16)\n",
            "Columns in 'Meeting Notes' BEFORE drop attempt: ['Id', 'Additional_Information__c', 'Background_Information__c', 'Company_Country__c', 'Company_Needs__c', 'Created_On__c', 'Follow_Up_Action__c', 'Lead__c', 'Meeting_Note_Migration_Ref_ID__c', 'Name', 'Minutes_of_Meetings__c', 'Organisation__c', 'OwnerId', 'Potential__c', 'RecordTypeId', 'Source__c']\n",
            "Dropped columns: ['Additional_Information__c', 'Background_Information__c', 'Meeting_Note_Migration_Ref_ID__c', 'Minutes_of_Meetings__c']\n",
            "New shape: (3181, 12)\n",
            "\n",
            "--- Running Cell 5: Updated Missing Value Analysis ---\n",
            "\n",
            "--- Missing Value Analysis for Company Accounts (Shape: (5631, 16)) ---\n",
            "Columns with missing values (count, %):\n",
            "  Name: 1 (0.02%)\n",
            "  Company_Profile__c: 1838 (32.64%)\n",
            "  Country__c: 117 (2.08%)\n",
            "  Nature_of_Business__c: 2717 (48.25%)\n",
            "  Industry: 62 (1.10%)\n",
            "  Primary_Industry__c: 1620 (28.77%)\n",
            "  AccountSource: 1894 (33.64%)\n",
            "  Lead_Source__c: 2269 (40.29%)\n",
            "  Year_of_First_Engagement__c: 14 (0.25%)\n",
            "\n",
            "--- Missing Value Analysis for Leads (Shape: (47627, 13)) ---\n",
            "Columns with missing values (count, %):\n",
            "  Address_country: 15591 (32.74%)\n",
            "  Company: 69 (0.14%)\n",
            "  LeadSource: 31384 (65.90%)\n",
            "  ConvertedAccountId: 46220 (97.05%)\n",
            "  ConvertedContactId: 46228 (97.06%)\n",
            "  ConvertedDate: 46177 (96.96%)\n",
            "  ConvertedOpportunityId: 46676 (98.00%)\n",
            "\n",
            "--- Missing Value Analysis for Contacts (Shape: (7374, 14)) ---\n",
            "Columns with missing values (count, %):\n",
            "  Areas_of_Interest__c: 2922 (39.63%)\n",
            "  AccountId: 45 (0.61%)\n",
            "  LeadSource: 3421 (46.39%)\n",
            "\n",
            "--- Missing Value Analysis for Projects (Shape: (4182, 3)) ---\n",
            "No missing values found.\n",
            "\n",
            "--- Missing Value Analysis for Meeting Notes (Shape: (3181, 12)) ---\n",
            "Columns with missing values (count, %):\n",
            "  Company_Country__c: 29 (0.91%)\n",
            "  Company_Needs__c: 1113 (34.99%)\n",
            "  Follow_Up_Action__c: 206 (6.48%)\n",
            "  Lead__c: 3135 (98.55%)\n",
            "  Organisation__c: 29 (0.91%)\n",
            "  Potential__c: 153 (4.81%)\n",
            "  RecordTypeId: 6 (0.19%)\n",
            "  Source__c: 1549 (48.70%)\n",
            "\n",
            "--- Running Cell 6: Row Deletion for False Company Names ---\n",
            "df_leads: Removed 15019 rows with false company names. New shape: (32608, 13)\n",
            "df_companyaccounts: Removed 0 rows with false names. New shape: (5631, 16)\n",
            "\n",
            "--- Running Cell 7: Merging Account Info into df_leads ---\n",
            "Found Account ID foreign key in df_leads: 'ConvertedAccountId'\n",
            "Attempting ID-based merge: df_leads['ConvertedAccountId'] with df_companyaccounts['Id']\n",
            "ID-based merge complete. df_leads shape: (32608, 20)\n",
            "Attempting name-based join/fill for remaining account info...\n",
            "Account info merge attempt complete.\n",
            "\n",
            "--- Running Cell 8: Impute df_leads ---\n",
            "\n",
            "--- Running Cell 9: Impute df_companyaccounts ---\n",
            "\n",
            "--- Running Cell 10: Impute df_contacts ---\n",
            "\n",
            "--- Running Cell 11: Impute df_meeting_notes ---\n",
            "WARNING: 'Lead__c' in Meeting Notes is still overwhelmingly 'Unknown_Lead_FK'.\n",
            "\n",
            "--- Running Final Cell: Feature Engineering, Target Creation, Encoding ---\n",
            "Added Lead_Created_Year, Month, DayOfWeek to df_leads.\n",
            "Added 'Account_Tenure_At_Lead_Creation' to df_leads.\n",
            "\n",
            "Created df_master from df_leads. Shape: (32608, 29)\n",
            "Creating 'is_converted' target variable directly from 'IsConverted' (from Excel merge).\n",
            "'is_converted' value counts:\n",
            "is_converted\n",
            "0    31158\n",
            "1     1450\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Encoding Categorical Features...\n",
            "\n",
            "--- Inspecting 'Status' column before encoding for data leakage ---\n",
            "Unique values in df_master['Status']: ['Engaged - KIV' 'Reached Out' 'Engaged - Drop'\n",
            " 'Declined before engagement' 'S0 - Drop' 'Engaged â€“ Drop' 'Converted'\n",
            " 'Engaged - Project Ended Prematurely' 'Engaged - Exploratory Phase'\n",
            " 'Engaged - Scoping Phase' 'New' 'S1 - Prospect' 'S2 - Project Scoped'\n",
            " 'S4 - Quote Signed' 'Active' 'Unsubscribed']\n",
            "WARNING: 'Status' column contains values (['Converted', 'Closed Won', 'Closed Lost', 'Disqualified - Final', 'Dropped - Final']) that might cause data leakage if one-hot encoded directly.\n",
            "Removing 'Status' from the list of features to be one-hot encoded.\n",
            "Attempting to one-hot encode: ['Industry', 'Address_country', 'LeadSource', 'IAP__c', 'Lead_RT_Name__c', 'Industry_FromAccount', 'Primary_Industry__c_FromAccount', 'Country__c_FromAccount', 'Company_Profile__c_FromAccount', 'AccountSource_FromAccount']\n",
            "Shape of df_master before encoding: (32608, 30)\n",
            "Shape of df_master after one-hot encoding: (32608, 310)\n",
            "\n",
            "\n",
            "--- All Processing Steps From Notebook Applied with Fixes ---\n",
            "Review df_master, especially the 'is_converted' column and its value counts.\n",
            "Ensure the value counts for 'is_converted' show both 0s and 1s.\n",
            "The original 'Industry' column in df_master (from leads) can now be imputed using Gemini API if desired, before model training.\n",
            "Next main steps: Further feature selection/scaling, data splitting, and model training.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Not used in this consolidated script's printouts\n",
        "import seaborn as sns # Not used in this consolidated script's printouts\n",
        "\n",
        "# --- Cell 1: Setup (Assuming BigQuery client 'client' is already set up) ---\n",
        "# This part should be run once to load your initial data.\n",
        "# If running this as a whole new script, you need to execute your BigQuery loading logic first.\n",
        "# Ensure df_companyaccounts, df_leads, etc., are loaded before proceeding.\n",
        "# Example (from your notebook, run this if DataFrames are not loaded):\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!pip install --quiet google-cloud-bigquery pandas openpyxl\n",
        "from google.cloud import bigquery\n",
        "PROJECT_ID = \"lead-conversion-prediction\"\n",
        "DATASET = \"lead_conversion_staging_tables\"\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "def load_table(table_name):\n",
        "    query = f\"SELECT * FROM `{PROJECT_ID}.{DATASET}.{table_name}`\"\n",
        "    return client.query(query).to_dataframe()\n",
        "print(\"Loading initial tables from BigQuery...\")\n",
        "df_companyaccounts = load_table(\"staging_companyaccounts\")\n",
        "df_leads = load_table(\"staging_leads\")\n",
        "df_contacts = load_table(\"staging_contacts\")\n",
        "df_projects = load_table(\"staging_projects\")\n",
        "df_meeting_notes = load_table(\"staging_meeting_notes\")\n",
        "print(\"Initial tables loaded.\")\n",
        "\n",
        "# --- Cell 2: Load and Merge Excel Data into df_leads ---\n",
        "print(\"\\n--- Running Cell 2: Load and Merge Excel Data into df_leads ---\")\n",
        "excel_file_path = 'Leads.xlsx' # Make sure this file is uploaded\n",
        "\n",
        "if 'df_leads' in locals() and isinstance(df_leads, pd.DataFrame):\n",
        "    try:\n",
        "        df_new_lead_info = pd.read_excel(excel_file_path)\n",
        "        print(f\"Successfully loaded data from '{excel_file_path}'. Shape: {df_new_lead_info.shape}\")\n",
        "        original_df_leads_cols = set(df_leads.columns)\n",
        "        df_leads = pd.merge(df_leads, df_new_lead_info, on='Id', how='left', suffixes=('', '_from_excel'))\n",
        "        newly_added_cols = list(set(df_leads.columns) - original_df_leads_cols)\n",
        "        print(f\"Merge complete. df_leads new shape: {df_leads.shape}\")\n",
        "        print(f\"Newly added/updated columns from Excel: {newly_added_cols}\")\n",
        "\n",
        "        if 'IsConverted' in df_leads.columns:\n",
        "            print(\"\\n'IsConverted' column (from Excel) is now in df_leads.\")\n",
        "            print(\"Value counts for 'IsConverted' (from Excel) in df_leads:\")\n",
        "            print(df_leads['IsConverted'].value_counts(dropna=False))\n",
        "        else:\n",
        "            print(\"\\nWARNING: 'IsConverted' (from Excel) column not found in df_leads after merge.\")\n",
        "        if 'ConvertedAccountId' in df_leads.columns:\n",
        "            print(\"\\n'ConvertedAccountId' (from Excel) is now in df_leads.\")\n",
        "        else:\n",
        "            print(\"\\nWARNING: 'ConvertedAccountId' (from Excel) column not found in df_leads after merge.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The Excel file '{excel_file_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading or merging Excel file: {e}\")\n",
        "else:\n",
        "    print(\"Error: df_leads DataFrame not found or not a DataFrame for Cell 2. Please ensure initial load from BQ is done.\")\n",
        "\n",
        "\n",
        "# --- Cell 3: Convert String NaNs ---\n",
        "print(\"\\n--- Running Cell 3: Convert String NaNs ---\")\n",
        "dataframes_dict_cell3 = {\n",
        "    \"Company Accounts\": df_companyaccounts if 'df_companyaccounts' in locals() and isinstance(df_companyaccounts, pd.DataFrame) else None,\n",
        "    \"Leads\": df_leads if 'df_leads' in locals() and isinstance(df_leads, pd.DataFrame) else None,\n",
        "    \"Contacts\": df_contacts if 'df_contacts' in locals() and isinstance(df_contacts, pd.DataFrame) else None,\n",
        "    \"Projects\": df_projects if 'df_projects' in locals() and isinstance(df_projects, pd.DataFrame) else None,\n",
        "    \"Meeting Notes\": df_meeting_notes if 'df_meeting_notes' in locals() and isinstance(df_meeting_notes, pd.DataFrame) else None\n",
        "}\n",
        "missing_value_formats = ['nan', 'NaN', 'Nan', 'NAN', 'None', 'NONE', 'null', 'NULL', '', ' ', '--', 'N/A', 'n/a', 'NA', 'na']\n",
        "for df_name, df_current in dataframes_dict_cell3.items():\n",
        "    if df_current is None: continue\n",
        "    for col in df_current.columns:\n",
        "        if df_current[col].dtype == 'object':\n",
        "            for fmt in missing_value_formats:\n",
        "                df_current.loc[df_current[col] == fmt, col] = np.nan\n",
        "\n",
        "# --- Cell 4: Define Columns to Drop ---\n",
        "print(\"\\n--- Running Cell 4: Define Columns to Drop ---\")\n",
        "cols_to_drop_companyaccounts = ['About_the_Company__c', 'Areas_of_Interest__c', 'IPI_Remarks__c', 'IPI_WAR_Rating__c', 'Main_Products_Services__c', 'Other_Sources__c', 'ParentId', 'Other_Primary_Industry__c', 'Ready__c', 'Revenue__c', 'Tech_Seeker_Type__c', 'Year_of_Last_Engagement__c']\n",
        "cols_to_drop_leads = ['Areas_of_Interest__c', 'Company_Classification__c', 'Company_Profile__c', 'Drop_Reason__c', 'Drop_Remarks__c', 'IPI_Remarks__c', 'Industry', 'Tech_Expert_Name__c', 'Tech_Need_Title__c', 'Tech_Offer_Title__c', 'Type_of_Enquiry__c', 'Type_of_Service__c']\n",
        "cols_to_drop_contacts = ['Country__c', 'CventEvents__ContactStub__c', 'Cvent_Reference_ID__c', 'Cvent_Registration_Type__c', 'Source__c']\n",
        "cols_to_drop_meeting_notes = ['Additional_Information__c', 'Background_Information__c', 'Meeting_Note_Migration_Ref_ID__c', 'Minutes_of_Meetings__c']\n",
        "\n",
        "def drop_columns_and_summarize_fixed(df, df_name, cols_to_drop):\n",
        "    if df is None: print(f\"DataFrame '{df_name}' is not loaded. Skipping drop.\"); return None\n",
        "    print(f\"\\n--- Dropping Columns from: {df_name} ---\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "    print(f\"Columns in '{df_name}' BEFORE drop attempt: {df.columns.tolist()}\")\n",
        "    existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
        "    if not existing_cols_to_drop:\n",
        "        print(f\"No specified columns from the drop list found in '{df_name}'.\")\n",
        "        df_modified = df.copy()\n",
        "    else:\n",
        "        df_modified = df.drop(columns=existing_cols_to_drop, errors='ignore')\n",
        "        print(f\"Dropped columns: {existing_cols_to_drop}\")\n",
        "    print(f\"New shape: {df_modified.shape}\")\n",
        "    return df_modified\n",
        "\n",
        "if 'df_companyaccounts' in locals(): df_companyaccounts = drop_columns_and_summarize_fixed(df_companyaccounts, \"Company Accounts\", cols_to_drop_companyaccounts)\n",
        "if 'df_leads' in locals(): df_leads = drop_columns_and_summarize_fixed(df_leads, \"Leads\", cols_to_drop_leads)\n",
        "if 'df_contacts' in locals(): df_contacts = drop_columns_and_summarize_fixed(df_contacts, \"Contacts\", cols_to_drop_contacts)\n",
        "if 'df_projects' in locals() and df_projects is not None:\n",
        "    print(f\"\\n--- Processing DataFrame for Keep: Projects ---\")\n",
        "    cols_to_keep_projects = []\n",
        "    possible_lead_fk_cols = ['LeadId', 'Lead__c', 'Created_From_Lead__c', 'Lead']\n",
        "    possible_conversion_cols = ['IsWon', 'is_converted', 'iswon']\n",
        "    if 'Id' in df_projects.columns: cols_to_keep_projects.append('Id')\n",
        "    lead_fk_found = any(col_name in df_projects.columns for col_name in possible_lead_fk_cols)\n",
        "    conversion_col_found = any(col_name in df_projects.columns for col_name in possible_conversion_cols)\n",
        "    if lead_fk_found: cols_to_keep_projects.extend([col for col in possible_lead_fk_cols if col in df_projects.columns])\n",
        "    if conversion_col_found: cols_to_keep_projects.extend([col for col in possible_conversion_cols if col in df_projects.columns])\n",
        "    cols_to_keep_projects = sorted(list(set(cols_to_keep_projects)))\n",
        "    if 'Id' in cols_to_keep_projects and \\\n",
        "       any(fk in cols_to_keep_projects for fk in possible_lead_fk_cols) and \\\n",
        "       any(conv in cols_to_keep_projects for conv in possible_conversion_cols):\n",
        "        print(f\"Identified columns to keep in Projects: {cols_to_keep_projects}\")\n",
        "        df_projects = df_projects[cols_to_keep_projects]\n",
        "    else:\n",
        "        print(f\"WARNING: Could not robustly identify all essential columns in df_projects. Kept: {cols_to_keep_projects}. Review manually.\")\n",
        "    print(f\"New shape of Projects: {df_projects.shape}\")\n",
        "else:\n",
        "    print(\"df_projects not available for column keeping.\")\n",
        "if 'df_meeting_notes' in locals(): df_meeting_notes = drop_columns_and_summarize_fixed(df_meeting_notes, \"Meeting Notes\", cols_to_drop_meeting_notes)\n",
        "\n",
        "# --- Cell 5: Updated Missing Value Analysis ---\n",
        "print(\"\\n--- Running Cell 5: Updated Missing Value Analysis ---\")\n",
        "dataframes_dict_cell5 = {\n",
        "    \"Company Accounts\": df_companyaccounts if 'df_companyaccounts' in locals() else None,\n",
        "    \"Leads\": df_leads if 'df_leads' in locals() else None,\n",
        "    \"Contacts\": df_contacts if 'df_contacts' in locals() else None,\n",
        "    \"Projects\": df_projects if 'df_projects' in locals() else None,\n",
        "    \"Meeting Notes\": df_meeting_notes if 'df_meeting_notes' in locals() else None\n",
        "}\n",
        "for df_name, df_current in dataframes_dict_cell5.items():\n",
        "    if df_current is None: continue\n",
        "    print(f\"\\n--- Missing Value Analysis for {df_name} (Shape: {df_current.shape}) ---\")\n",
        "    missing_values = df_current.isnull().sum()\n",
        "    missing_cols = missing_values[missing_values > 0]\n",
        "    if not missing_cols.empty:\n",
        "        print(\"Columns with missing values (count, %):\")\n",
        "        for col, count in missing_cols.items():\n",
        "            print(f\"  {col}: {count} ({ (count/len(df_current)*100):.2f}%)\")\n",
        "    else:\n",
        "        print(\"No missing values found.\")\n",
        "\n",
        "# --- Cell 6: Row Deletion for False Company Names ---\n",
        "print(\"\\n--- Running Cell 6: Row Deletion for False Company Names ---\")\n",
        "false_company_names_to_delete = [\"83\", \"99\", \"111234\", \"-\", \"[Company]\"]\n",
        "if 'df_leads' in locals() and df_leads is not None:\n",
        "    leads_company_col = 'Company'\n",
        "    if leads_company_col in df_leads.columns:\n",
        "        original_lead_count = len(df_leads)\n",
        "        df_leads[leads_company_col] = df_leads[leads_company_col].astype(str)\n",
        "        df_leads = df_leads[~df_leads[leads_company_col].isin(false_company_names_to_delete)]\n",
        "        print(f\"df_leads: Removed {original_lead_count - len(df_leads)} rows with false company names. New shape: {df_leads.shape}\")\n",
        "    else: print(f\"Column '{leads_company_col}' not found in df_leads.\")\n",
        "else: print(\"df_leads not available for Cell 6.\")\n",
        "\n",
        "if 'df_companyaccounts' in locals() and df_companyaccounts is not None:\n",
        "    accounts_name_col = 'Name'\n",
        "    if accounts_name_col in df_companyaccounts.columns:\n",
        "        original_account_count = len(df_companyaccounts)\n",
        "        df_companyaccounts[accounts_name_col] = df_companyaccounts[accounts_name_col].astype(str)\n",
        "        df_companyaccounts = df_companyaccounts[~df_companyaccounts[accounts_name_col].isin(false_company_names_to_delete)]\n",
        "        print(f\"df_companyaccounts: Removed {original_account_count - len(df_companyaccounts)} rows with false names. New shape: {df_companyaccounts.shape}\")\n",
        "    else: print(f\"Column '{accounts_name_col}' not found in df_companyaccounts.\")\n",
        "else: print(\"df_companyaccounts not available for Cell 6.\")\n",
        "\n",
        "\n",
        "# --- Cell 7: Merging Account Info into df_leads ---\n",
        "print(\"\\n--- Running Cell 7: Merging Account Info into df_leads ---\")\n",
        "if ('df_leads' in locals() and df_leads is not None) and \\\n",
        "   ('df_companyaccounts' in locals() and df_companyaccounts is not None):\n",
        "    accounts_pk_col = 'Id'\n",
        "    accounts_name_col = 'Name'\n",
        "    leads_company_name_col = 'Company'\n",
        "    possible_leads_fk_to_accounts = ['ConvertedAccountId', 'AccountId', 'Account_Id']\n",
        "    actual_leads_fk_col = None\n",
        "    for col_name in possible_leads_fk_to_accounts:\n",
        "        if col_name in df_leads.columns: actual_leads_fk_col = col_name; break\n",
        "\n",
        "    if actual_leads_fk_col: print(f\"Found Account ID foreign key in df_leads: '{actual_leads_fk_col}'\")\n",
        "    else: print(f\"WARNING: No prioritized Account ID FK found in df_leads from {possible_leads_fk_to_accounts}.\")\n",
        "\n",
        "    cols_from_accounts_to_merge = ['Id', 'Industry', 'Primary_Industry__c', 'Country__c', 'Company_Profile__c', 'Year_of_First_Engagement__c', 'AccountSource']\n",
        "    cols_from_accounts_to_merge = [col for col in cols_from_accounts_to_merge if col in df_companyaccounts.columns]\n",
        "    df_accounts_subset = df_companyaccounts[cols_from_accounts_to_merge].copy()\n",
        "\n",
        "    if actual_leads_fk_col and accounts_pk_col in df_accounts_subset.columns:\n",
        "        print(f\"Attempting ID-based merge: df_leads['{actual_leads_fk_col}'] with df_companyaccounts['{accounts_pk_col}']\")\n",
        "        df_leads = pd.merge(df_leads, df_accounts_subset, left_on=actual_leads_fk_col, right_on=accounts_pk_col, how='left', suffixes=('', '_FromAccount'))\n",
        "        print(f\"ID-based merge complete. df_leads shape: {df_leads.shape}\")\n",
        "    else:\n",
        "        print(\"Skipped ID-based merge. Conditions not met (e.g., FK missing in leads or PK in accounts).\")\n",
        "\n",
        "    if leads_company_name_col in df_leads.columns and accounts_name_col in df_companyaccounts.columns:\n",
        "        print(\"Attempting name-based join/fill for remaining account info...\")\n",
        "        df_leads.loc[:, 'Clean_Lead_Company_Name'] = df_leads[leads_company_name_col].astype(str).str.lower().str.strip()\n",
        "        df_companyaccounts.loc[:, 'Clean_Account_Name'] = df_companyaccounts[accounts_name_col].astype(str).str.lower().str.strip()\n",
        "        df_accounts_for_map = df_companyaccounts.drop_duplicates(subset=['Clean_Account_Name'], keep='first')\n",
        "        for acc_col in cols_from_accounts_to_merge:\n",
        "            if acc_col == accounts_pk_col: continue\n",
        "            target_col_in_leads = acc_col + '_FromAccount'\n",
        "            if target_col_in_leads not in df_leads.columns: df_leads[target_col_in_leads] = np.nan\n",
        "            df_leads[target_col_in_leads] = df_leads[target_col_in_leads].astype('object')\n",
        "            if acc_col in df_accounts_for_map.columns:\n",
        "                name_to_feature_map = df_accounts_for_map.set_index('Clean_Account_Name')[acc_col]\n",
        "                condition_to_fill = df_leads[target_col_in_leads].isnull()\n",
        "                if condition_to_fill.any():\n",
        "                    values_to_map = df_leads.loc[condition_to_fill, 'Clean_Lead_Company_Name'].map(name_to_feature_map)\n",
        "                    df_leads.loc[condition_to_fill, target_col_in_leads] = df_leads.loc[condition_to_fill, target_col_in_leads].fillna(values_to_map)\n",
        "        if 'Clean_Lead_Company_Name' in df_leads.columns: df_leads.drop(columns=['Clean_Lead_Company_Name'], inplace=True)\n",
        "    if accounts_pk_col + '_FromAccount' in df_leads.columns and actual_leads_fk_col != accounts_pk_col + '_FromAccount':\n",
        "        df_leads = df_leads.drop(columns=[accounts_pk_col + '_FromAccount'])\n",
        "    print(\"Account info merge attempt complete.\")\n",
        "else:\n",
        "    print(\"Error: df_leads or df_companyaccounts not found for Cell 7.\")\n",
        "\n",
        "# --- Cells 8, 9, 10, 11: Imputations ---\n",
        "print(\"\\n--- Running Cell 8: Impute df_leads ---\")\n",
        "if 'df_leads' in locals() and df_leads is not None:\n",
        "    cols_to_impute_leads = {'Address_country': 'Unknown_Country', 'Company': 'Unknown_Company', 'LeadSource': 'Unknown_Source'}\n",
        "    for col, fill_val in cols_to_impute_leads.items():\n",
        "        if col in df_leads.columns and df_leads[col].isnull().any():\n",
        "            df_leads[col] = df_leads[col].fillna(fill_val)\n",
        "else: print(\"df_leads not found for Cell 8.\")\n",
        "\n",
        "print(\"\\n--- Running Cell 9: Impute df_companyaccounts ---\")\n",
        "if 'df_companyaccounts' in locals() and df_companyaccounts is not None:\n",
        "    cols_to_impute_cat_accounts = {'Name': 'Unknown_Account_Name', 'Company_Profile__c': 'Unknown_Profile', 'Country__c': 'Unknown_Country', 'Nature_of_Business__c': 'Unknown_Nature', 'Industry': 'Unknown_Industry', 'Primary_Industry__c': 'Unknown_Primary_Industry', 'AccountSource': 'Unknown_Source', 'Lead_Source__c': 'Unknown_Source'}\n",
        "    for col, fill_val in cols_to_impute_cat_accounts.items():\n",
        "        if col in df_companyaccounts.columns and df_companyaccounts[col].isnull().any():\n",
        "            df_companyaccounts[col] = df_companyaccounts[col].fillna(fill_val)\n",
        "    if 'Year_of_First_Engagement__c' in df_companyaccounts.columns and df_companyaccounts['Year_of_First_Engagement__c'].isnull().any():\n",
        "        df_companyaccounts['Year_of_First_Engagement__c'] = pd.to_numeric(df_companyaccounts['Year_of_First_Engagement__c'], errors='coerce')\n",
        "        df_companyaccounts['Year_of_First_Engagement__c'] = df_companyaccounts['Year_of_First_Engagement__c'].fillna(df_companyaccounts['Year_of_First_Engagement__c'].median())\n",
        "else: print(\"df_companyaccounts not found for Cell 9.\")\n",
        "\n",
        "print(\"\\n--- Running Cell 10: Impute df_contacts ---\")\n",
        "if 'df_contacts' in locals() and df_contacts is not None:\n",
        "    cols_to_impute_contacts = {'Areas_of_Interest__c': 'Unknown_Interest', 'AccountId': 'Unknown_Account_FK', 'LeadSource': 'Unknown_Source'}\n",
        "    for col, fill_val in cols_to_impute_contacts.items():\n",
        "        if col in df_contacts.columns and df_contacts[col].isnull().any():\n",
        "            df_contacts[col] = df_contacts[col].fillna(fill_val)\n",
        "else: print(\"df_contacts not found for Cell 10.\")\n",
        "\n",
        "print(\"\\n--- Running Cell 11: Impute df_meeting_notes ---\")\n",
        "if 'df_meeting_notes' in locals() and df_meeting_notes is not None:\n",
        "    cols_to_impute_meeting = {'Company_Country__c': 'Unknown_Country', 'Company_Needs__c': 'Unknown_Needs', 'Follow_Up_Action__c': 'No_Action_Specified', 'Lead__c': 'Unknown_Lead_FK', 'Organisation__c': 'Unknown_Organisation_FK', 'Potential__c': 'Unknown_Potential', 'RecordTypeId': 'Unknown_RecordType', 'Source__c': 'Unknown_Source'}\n",
        "    for col, fill_val in cols_to_impute_meeting.items():\n",
        "        if col in df_meeting_notes.columns and df_meeting_notes[col].isnull().any():\n",
        "            df_meeting_notes[col] = df_meeting_notes[col].fillna(fill_val)\n",
        "    if 'Lead__c' in df_meeting_notes.columns and (df_meeting_notes['Lead__c'] == 'Unknown_Lead_FK').sum() / len(df_meeting_notes) > 0.9:\n",
        "         print(\"WARNING: 'Lead__c' in Meeting Notes is still overwhelmingly 'Unknown_Lead_FK'.\")\n",
        "else: print(\"df_meeting_notes not found for Cell 11.\")\n",
        "\n",
        "# --- Final Cell (Feature Engineering, Target Variable, Encoding - REVISED TARGET LOGIC) ---\n",
        "print(\"\\n--- Running Final Cell: Feature Engineering, Target Creation, Encoding ---\")\n",
        "\n",
        "# Initialize variables that might be defined conditionally\n",
        "projects_lead_fk_col = None\n",
        "projects_target_col = None\n",
        "target_col_in_master = None # For the suffixed version from project merge\n",
        "\n",
        "if 'df_leads' in locals() and df_leads is not None:\n",
        "    # A. Date Features\n",
        "    if 'CreatedDate' in df_leads.columns:\n",
        "        if not pd.api.types.is_datetime64_any_dtype(df_leads['CreatedDate']):\n",
        "            df_leads['CreatedDate'] = pd.to_datetime(df_leads['CreatedDate'], errors='coerce')\n",
        "        if 'Lead_Created_Year' not in df_leads.columns:\n",
        "            df_leads['Lead_Created_Year'] = df_leads['CreatedDate'].dt.year\n",
        "            df_leads['Lead_Created_Month'] = df_leads['CreatedDate'].dt.month\n",
        "            df_leads['Lead_Created_DayOfWeek'] = df_leads['CreatedDate'].dt.dayofweek\n",
        "            print(\"Added Lead_Created_Year, Month, DayOfWeek to df_leads.\")\n",
        "\n",
        "    # B. Account Tenure\n",
        "    account_tenure_col_name = 'Year_of_First_Engagement__c_FromAccount'\n",
        "    if account_tenure_col_name in df_leads.columns and 'Lead_Created_Year' in df_leads.columns:\n",
        "        df_leads[account_tenure_col_name] = pd.to_numeric(df_leads[account_tenure_col_name], errors='coerce')\n",
        "        df_leads['Account_Tenure_At_Lead_Creation'] = df_leads['Lead_Created_Year'] - df_leads[account_tenure_col_name]\n",
        "        df_leads.loc[df_leads['Account_Tenure_At_Lead_Creation'] < 0, 'Account_Tenure_At_Lead_Creation'] = 0\n",
        "        print(\"Added 'Account_Tenure_At_Lead_Creation' to df_leads.\")\n",
        "    else:\n",
        "        print(f\"Could not create 'Account_Tenure_At_Lead_Creation': '{account_tenure_col_name}' or 'Lead_Created_Year' missing.\")\n",
        "\n",
        "    df_master = df_leads.copy()\n",
        "    print(f\"\\nCreated df_master from df_leads. Shape: {df_master.shape}\")\n",
        "\n",
        "    # --- Target Variable Creation (Prioritizing 'IsConverted' from Excel) ---\n",
        "    if 'IsConverted' in df_master.columns:\n",
        "        print(\"Creating 'is_converted' target variable directly from 'IsConverted' (from Excel merge).\")\n",
        "        df_master['is_converted'] = df_master['IsConverted'].fillna(False).astype(bool).astype(int)\n",
        "    elif 'df_projects' in locals() and df_projects is not None:\n",
        "        print(\"WARNING: 'IsConverted' (from Excel) not in df_master. Attempting to use df_projects.\")\n",
        "        possible_lead_fk_cols_proj = ['Created_From_Lead__c', 'LeadId', 'Lead__c']\n",
        "        for col in possible_lead_fk_cols_proj:\n",
        "            if col in df_projects.columns: projects_lead_fk_col = col; break\n",
        "        projects_target_col = 'IsWon'\n",
        "\n",
        "        if projects_lead_fk_col and projects_target_col in df_projects.columns and 'Id' in df_master.columns:\n",
        "            print(f\"Attempting to merge df_projects into df_master using: df_master['Id'] and df_projects['{projects_lead_fk_col}']\")\n",
        "            df_projects_subset = df_projects[[projects_lead_fk_col, projects_target_col]].copy()\n",
        "            df_master = pd.merge(df_master, df_projects_subset, left_on='Id', right_on=projects_lead_fk_col, how='left', suffixes=('', '_Project'))\n",
        "            target_col_in_master = projects_target_col + '_Project' if projects_target_col + '_Project' in df_master.columns else projects_target_col\n",
        "            if target_col_in_master in df_master.columns:\n",
        "                df_master['is_converted'] = df_master[target_col_in_master].fillna(False).astype(bool).astype(int)\n",
        "                print(f\"Created 'is_converted' from df_projects '{target_col_in_master}'.\")\n",
        "            else:\n",
        "                print(\"CRITICAL WARNING: Could not create target 'is_converted' from df_projects. Defaulting to 0.\")\n",
        "                df_master['is_converted'] = 0\n",
        "        else:\n",
        "            print(\"CRITICAL WARNING: Key columns missing for df_projects merge. Defaulting 'is_converted' to 0.\")\n",
        "            df_master['is_converted'] = 0\n",
        "    else:\n",
        "        print(\"CRITICAL WARNING: No source for target variable ('IsConverted' or df_projects). 'is_converted' defaulted to 0.\")\n",
        "        df_master['is_converted'] = 0\n",
        "\n",
        "    print(\"'is_converted' value counts:\")\n",
        "    print(df_master['is_converted'].value_counts(dropna=False))\n",
        "\n",
        "    # Clean up intermediate project columns if they exist from merge\n",
        "    project_cols_merged = []\n",
        "    for col_name_iter in df_master.columns:\n",
        "        is_project_col = '_Project' in col_name_iter\n",
        "        if projects_lead_fk_col is not None:\n",
        "            is_project_col = is_project_col or (col_name_iter == projects_lead_fk_col)\n",
        "        if projects_target_col is not None:\n",
        "            is_project_col = is_project_col or (col_name_iter == projects_target_col)\n",
        "        if target_col_in_master is not None and target_col_in_master != projects_target_col: # Check the suffixed version too\n",
        "             is_project_col = is_project_col or (col_name_iter == target_col_in_master)\n",
        "        if is_project_col:\n",
        "            project_cols_merged.append(col_name_iter)\n",
        "\n",
        "    project_cols_to_drop = [col for col in project_cols_merged if col != 'is_converted' and col != 'Id' and col != 'IsConverted']\n",
        "    if project_cols_to_drop:\n",
        "        df_master = df_master.drop(columns=project_cols_to_drop, errors='ignore')\n",
        "        print(f\"Cleaned up project-related columns: {project_cols_to_drop}\")\n",
        "\n",
        "    # --- Encoding Categorical Features ---\n",
        "    print(\"\\nEncoding Categorical Features...\")\n",
        "    categorical_features_to_encode = [\n",
        "        'Industry', 'Address_country', 'LeadSource', 'Status', 'IAP__c', 'Lead_RT_Name__c',\n",
        "        'Industry_FromAccount', 'Primary_Industry__c_FromAccount', 'Country__c_FromAccount',\n",
        "        'Company_Profile__c_FromAccount', 'AccountSource_FromAccount'\n",
        "    ]\n",
        "    existing_categorical_features = [col for col in categorical_features_to_encode if col in df_master.columns]\n",
        "\n",
        "    # Data Leakage Prevention for 'Status' column\n",
        "    if 'Status' in existing_categorical_features:\n",
        "        print(\"\\n--- Inspecting 'Status' column before encoding for data leakage ---\")\n",
        "        status_unique_values = df_master['Status'].unique()\n",
        "        print(f\"Unique values in df_master['Status']: {status_unique_values}\")\n",
        "        # Define status values that represent a completed/final state (leakage)\n",
        "        # Adjust this list based on your actual status values!\n",
        "        problematic_status_values = ['Converted', 'Closed Won', 'Closed Lost', 'Disqualified - Final', 'Dropped - Final']\n",
        "\n",
        "        leakage_found = any(status_val in status_unique_values for status_val in problematic_status_values)\n",
        "\n",
        "        if leakage_found:\n",
        "            print(f\"WARNING: 'Status' column contains values ({problematic_status_values}) that might cause data leakage if one-hot encoded directly.\")\n",
        "            print(\"Removing 'Status' from the list of features to be one-hot encoded.\")\n",
        "            existing_categorical_features.remove('Status')\n",
        "        else:\n",
        "            print(\"'Status' column does not seem to contain obvious post-conversion leakage terms based on the check list.\")\n",
        "\n",
        "    if existing_categorical_features:\n",
        "        print(f\"Attempting to one-hot encode: {existing_categorical_features}\")\n",
        "        if 'IAP__c' in existing_categorical_features and pd.api.types.is_bool_dtype(df_master['IAP__c']):\n",
        "            df_master['IAP__c'] = df_master['IAP__c'].astype(str)\n",
        "\n",
        "        original_shape = df_master.shape\n",
        "        df_master = pd.get_dummies(df_master, columns=existing_categorical_features, dummy_na=False, drop_first=False)\n",
        "        print(f\"Shape of df_master before encoding: {original_shape}\")\n",
        "        print(f\"Shape of df_master after one-hot encoding: {df_master.shape}\")\n",
        "    else:\n",
        "        print(\"No specified categorical features found in df_master for encoding.\")\n",
        "else:\n",
        "    print(\"df_leads not available. Cannot proceed with final feature engineering and encoding.\")\n",
        "\n",
        "print(\"\\n\\n--- All Processing Steps From Notebook Applied with Fixes ---\")\n",
        "print(\"Review df_master, especially the 'is_converted' column and its value counts.\")\n",
        "print(\"Ensure the value counts for 'is_converted' show both 0s and 1s.\")\n",
        "print(\"The original 'Industry' column in df_master (from leads) can now be imputed using Gemini API if desired, before model training.\")\n",
        "print(\"Next main steps: Further feature selection/scaling, data splitting, and model training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- Assume X_train, X_test, y_train, y_test are from your data splitting step ---\n",
        "\n",
        "if 'X_train' in locals() and 'X_test' in locals() and \\\n",
        "   'y_train' in locals() and 'y_test' in locals() and \\\n",
        "   isinstance(X_train, pd.DataFrame) and isinstance(X_test, pd.DataFrame):\n",
        "\n",
        "    print(\"--- Step 0: Final Feature Preparation (Revised for String to Float Error) ---\")\n",
        "\n",
        "    X_train_model_ready = X_train.copy()\n",
        "    X_test_model_ready = X_test.copy()\n",
        "\n",
        "    # --- Step 1: Feature Scaling (for Numerical Features) ---\n",
        "    print(\"\\n--- Feature Scaling (for Numerical Features) ---\")\n",
        "    numerical_features_to_scale = [\n",
        "        'Lead_Created_Year', 'Lead_Created_Month', 'Lead_Created_DayOfWeek',\n",
        "        'Account_Tenure_At_Lead_Creation'\n",
        "    ]\n",
        "    numerical_features_present = [col for col in numerical_features_to_scale if col in X_train_model_ready.columns]\n",
        "\n",
        "    if numerical_features_present:\n",
        "        print(f\"Identified numerical features for scaling: {numerical_features_present}\")\n",
        "        # Ensure these columns are purely numeric BEFORE scaling\n",
        "        for col in numerical_features_present:\n",
        "            if X_train_model_ready[col].dtype == 'object':\n",
        "                X_train_model_ready[col] = pd.to_numeric(X_train_model_ready[col], errors='coerce')\n",
        "                X_test_model_ready[col] = pd.to_numeric(X_test_model_ready[col], errors='coerce')\n",
        "                # Impute NaNs created by coerce\n",
        "                if X_train_model_ready[col].isnull().any():\n",
        "                    median_val = X_train_model_ready[col].median() # Median from train\n",
        "                    X_train_model_ready[col].fillna(median_val, inplace=True)\n",
        "                    X_test_model_ready[col].fillna(median_val, inplace=True)\n",
        "            elif X_train_model_ready[col].isnull().any(): # If already numeric but has NaNs\n",
        "                    median_val = X_train_model_ready[col].median()\n",
        "                    X_train_model_ready[col].fillna(median_val, inplace=True)\n",
        "                    X_test_model_ready[col].fillna(median_val, inplace=True)\n",
        "\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_model_ready[numerical_features_present] = scaler.fit_transform(X_train_model_ready[numerical_features_present])\n",
        "        X_test_model_ready[numerical_features_present] = scaler.transform(X_test_model_ready[numerical_features_present])\n",
        "        print(\"Numerical features scaled successfully.\")\n",
        "    else:\n",
        "        print(\"No explicitly listed numerical features found for scaling.\")\n",
        "\n",
        "    # --- Rigorous Check and Conversion/Dropping of Non-Numeric Columns BEFORE FIT ---\n",
        "    print(\"\\nPerforming rigorous check and handling of non-numeric columns in X_train_model_ready...\")\n",
        "    cols_to_drop_non_numeric = []\n",
        "    for col in X_train_model_ready.columns:\n",
        "        # If column is object, it means it wasn't one-hot encoded or a properly processed numeric feature\n",
        "        if X_train_model_ready[col].dtype == 'object':\n",
        "            print(f\"WARNING: Column '{col}' is object type. It was not one-hot encoded or converted to numeric.\")\n",
        "            # Attempt to convert to numeric. If it fails (still object or many NaNs), mark for dropping.\n",
        "            X_train_model_ready[col] = pd.to_numeric(X_train_model_ready[col], errors='coerce')\n",
        "            if X_test_model_ready.get(col) is not None: # Ensure col exists in X_test_model_ready\n",
        "                 X_test_model_ready[col] = pd.to_numeric(X_test_model_ready.get(col), errors='coerce')\n",
        "\n",
        "            # If after coerce it's still object or mostly NaNs, it's problematic\n",
        "            if X_train_model_ready[col].dtype == 'object' or X_train_model_ready[col].isnull().sum() > 0.9 * len(X_train_model_ready):\n",
        "                cols_to_drop_non_numeric.append(col)\n",
        "                print(f\"  Marking '{col}' for dropping as it could not be reliably converted to numeric.\")\n",
        "\n",
        "        # Also check for datetime types explicitly\n",
        "        elif pd.api.types.is_datetime64_any_dtype(X_train_model_ready[col]):\n",
        "            cols_to_drop_non_numeric.append(col)\n",
        "            print(f\"  Marking datetime column '{col}' for dropping.\")\n",
        "\n",
        "    if cols_to_drop_non_numeric:\n",
        "        print(f\"Dropping problematic non-numeric/datetime columns: {list(set(cols_to_drop_non_numeric))}\") # Use set to avoid duplicates\n",
        "        X_train_model_ready = X_train_model_ready.drop(columns=list(set(cols_to_drop_non_numeric)))\n",
        "        X_test_model_ready = X_test_model_ready.drop(columns=[col for col in list(set(cols_to_drop_non_numeric)) if col in X_test_model_ready.columns])\n",
        "        print(f\"Shape of X_train_model_ready after dropping non-numeric: {X_train_model_ready.shape}\")\n",
        "    else:\n",
        "        print(\"No problematic non-numeric columns found needing drop before final NaN imputation.\")\n",
        "\n",
        "    # --- Final NaN Imputation Step for ALL remaining columns (should be numeric now) ---\n",
        "    print(\"\\nPerforming final NaN check and imputation on X_train_model_ready...\")\n",
        "    if X_train_model_ready.isnull().any().any():\n",
        "        nan_columns_final = X_train_model_ready.columns[X_train_model_ready.isnull().any()].tolist()\n",
        "        print(f\"NaNs found in these columns before final imputation: {nan_columns_final}\")\n",
        "\n",
        "        # Ensure all columns are numeric before SimpleImputer\n",
        "        for col in nan_columns_final:\n",
        "            if not pd.api.types.is_numeric_dtype(X_train_model_ready[col]):\n",
        "                 X_train_model_ready[col] = pd.to_numeric(X_train_model_ready[col], errors='coerce')\n",
        "                 if col in X_test_model_ready.columns:\n",
        "                    X_test_model_ready[col] = pd.to_numeric(X_test_model_ready[col], errors='coerce')\n",
        "\n",
        "        imputer_final = SimpleImputer(strategy='median')\n",
        "        X_train_model_ready = pd.DataFrame(imputer_final.fit_transform(X_train_model_ready), columns=X_train_model_ready.columns, index=X_train_model_ready.index)\n",
        "        X_test_model_ready = pd.DataFrame(imputer_final.transform(X_test_model_ready), columns=X_test_model_ready.columns, index=X_test_model_ready.index)\n",
        "        print(f\"NaNs imputed using median for columns: {nan_columns_final}\")\n",
        "    else:\n",
        "        print(\"No NaNs found in X_train_model_ready before fitting model.\")\n",
        "\n",
        "    print(f\"Final NaN check in X_train_model_ready: {X_train_model_ready.isnull().sum().sum()}\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Step 2: Training Logistic Regression Model ---\")\n",
        "    log_reg_model = LogisticRegression(\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        solver='liblinear',\n",
        "        max_iter=1000\n",
        "    )\n",
        "\n",
        "    if X_train_model_ready.isnull().sum().sum() > 0:\n",
        "        print(\"CRITICAL ERROR: NaNs still present in X_train_model_ready before fitting. Aborting training.\")\n",
        "        print(X_train_model_ready.isnull().sum()[X_train_model_ready.isnull().sum() > 0])\n",
        "    else:\n",
        "        print(\"Training the Logistic Regression model...\")\n",
        "        log_reg_model.fit(X_train_model_ready, y_train)\n",
        "        print(\"Model training complete.\")\n",
        "\n",
        "        print(\"\\n--- Step 3: Evaluating the Model ---\")\n",
        "        # Ensure X_test_model_ready has the same columns as X_train_model_ready\n",
        "        X_test_model_ready = X_test_model_ready[X_train_model_ready.columns]\n",
        "\n",
        "        y_pred_log_reg = log_reg_model.predict(X_test_model_ready)\n",
        "        y_pred_proba_log_reg = log_reg_model.predict_proba(X_test_model_ready)[:, 1]\n",
        "\n",
        "        # ... (Rest of your evaluation metric calculations and printouts) ...\n",
        "        accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "        precision = precision_score(y_test, y_pred_log_reg)\n",
        "        recall = recall_score(y_test, y_pred_log_reg)\n",
        "        f1 = f1_score(y_test, y_pred_log_reg)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba_log_reg)\n",
        "\n",
        "        print(\"\\nLogistic Regression Performance Metrics:\")\n",
        "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall:    {recall:.4f}\")\n",
        "        print(f\"  F1-Score:  {f1:.4f}\")\n",
        "        print(f\"  AUC-ROC:   {roc_auc:.4f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        cm = confusion_matrix(y_test, y_pred_log_reg)\n",
        "        cm_df = pd.DataFrame(cm, index=['Actual Negative (0)', 'Actual Positive (1)'],\n",
        "                             columns=['Predicted Negative (0)', 'Predicted Positive (1)'])\n",
        "        print(cm_df)\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "        print(\"\\n--- Feature Importances (Coefficients for Logistic Regression) ---\")\n",
        "        try:\n",
        "            coefficients = pd.DataFrame({\n",
        "                'Feature': X_train_model_ready.columns, # Use columns from the actual fitted data\n",
        "                'Coefficient': log_reg_model.coef_[0]\n",
        "            })\n",
        "            coefficients['Absolute_Coefficient'] = coefficients['Coefficient'].abs()\n",
        "            coefficients_sorted = coefficients.sort_values(by='Absolute_Coefficient', ascending=False)\n",
        "            print(\"Top 20 most influential features:\")\n",
        "            print(coefficients_sorted.head(20))\n",
        "        except Exception as e:\n",
        "            print(f\"Could not display coefficients: {e}\")\n",
        "else:\n",
        "    print(\"Error: X_train, X_test, y_train, or y_test not found or not pandas DataFrames.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVSR83fV3UqL",
        "outputId": "ff5122ac-0d7a-4189-e4b2-6e7f1025da04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 0: Final Feature Preparation (Revised for String to Float Error) ---\n",
            "\n",
            "--- Feature Scaling (for Numerical Features) ---\n",
            "Identified numerical features for scaling: ['Lead_Created_Year', 'Lead_Created_Month', 'Lead_Created_DayOfWeek', 'Account_Tenure_At_Lead_Creation']\n",
            "Numerical features scaled successfully.\n",
            "\n",
            "Performing rigorous check and handling of non-numeric columns in X_train_model_ready...\n",
            "WARNING: Column 'Status' is object type. It was not one-hot encoded or converted to numeric.\n",
            "  Marking 'Status' for dropping as it could not be reliably converted to numeric.\n",
            "Dropping problematic non-numeric/datetime columns: ['Status']\n",
            "Shape of X_train_model_ready after dropping non-numeric: (26086, 295)\n",
            "\n",
            "Performing final NaN check and imputation on X_train_model_ready...\n",
            "NaNs found in these columns before final imputation: ['Year_of_First_Engagement__c_FromAccount']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f8d1b8813bc5>:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train_model_ready[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-6-f8d1b8813bc5>:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test_model_ready[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs imputed using median for columns: ['Year_of_First_Engagement__c_FromAccount']\n",
            "Final NaN check in X_train_model_ready: 0\n",
            "\n",
            "--- Step 2: Training Logistic Regression Model ---\n",
            "Training the Logistic Regression model...\n",
            "Model training complete.\n",
            "\n",
            "--- Step 3: Evaluating the Model ---\n",
            "\n",
            "Logistic Regression Performance Metrics:\n",
            "  Accuracy:  0.9951\n",
            "  Precision: 0.9108\n",
            "  Recall:    0.9862\n",
            "  F1-Score:  0.9470\n",
            "  AUC-ROC:   0.9992\n",
            "\n",
            "Confusion Matrix:\n",
            "                     Predicted Negative (0)  Predicted Positive (1)\n",
            "Actual Negative (0)                    6204                      28\n",
            "Actual Positive (1)                       4                     286\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      6232\n",
            "           1       0.91      0.99      0.95       290\n",
            "\n",
            "    accuracy                           1.00      6522\n",
            "   macro avg       0.96      0.99      0.97      6522\n",
            "weighted avg       1.00      1.00      1.00      6522\n",
            "\n",
            "\n",
            "--- Feature Importances (Coefficients for Logistic Regression) ---\n",
            "Top 20 most influential features:\n",
            "                                    Feature  Coefficient  Absolute_Coefficient\n",
            "5             Industry_Academia & Education     7.534317              7.534317\n",
            "23                          Industry_Others     7.125733              7.125733\n",
            "17                      Industry_Healthcare     6.890265              6.890265\n",
            "20     Industry_Manufacturing & Engineering     6.566898              6.566898\n",
            "11                     Industry_Electronics     6.415765              6.415765\n",
            "19                        Industry_Infocomm     6.324314              6.324314\n",
            "8          Industry_Building & Construction     6.177342              6.177342\n",
            "15                            Industry_Food     6.098044              6.098044\n",
            "12                          Industry_Energy     6.081962              6.081962\n",
            "9                        Industry_Chemicals     6.018123              6.018123\n",
            "26          Industry_Research & Development     5.986477              5.986477\n",
            "22                       Industry_Materials     5.893234              5.893234\n",
            "16                      Industry_Government     5.794437              5.794437\n",
            "7        Industry_Agriculture & Aquaculture     5.665189              5.665189\n",
            "27           Industry_Transport & Logistics     5.631024              5.631024\n",
            "13                     Industry_Environment     5.470853              5.470853\n",
            "207          Lead_RT_Name__c_IPI Subscriber    -4.983688              4.983688\n",
            "191  LeadSource_Sustainability Forward 2025     4.894280              4.894280\n",
            "25           Industry_Professional Services     4.844318              4.844318\n",
            "183               LeadSource_Portal Enquiry    -4.791241              4.791241\n"
          ]
        }
      ]
    }
  ]
}